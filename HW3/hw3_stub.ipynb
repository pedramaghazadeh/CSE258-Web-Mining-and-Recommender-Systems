{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62c3f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "import scipy.optimize\n",
    "import numpy\n",
    "import string\n",
    "from sklearn import linear_model\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74ac9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readGz(path):\n",
    "    for l in gzip.open(path, 'rt'):\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6d8628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCSV(path):\n",
    "    f = gzip.open(path, 'rt')\n",
    "    f.readline()\n",
    "    for l in f:\n",
    "        u,b,r = l.strip().split(',')\n",
    "        r = int(r)\n",
    "        yield u,b,r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f85871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    if denom > 0:\n",
    "        return numer/denom\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c51f68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Rating prediction                              #\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4c9854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGlobalAverage(trainRatings):\n",
    "    # Return the average rating in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e2d4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trivialValidMSE(ratingsValid, globalAverage):\n",
    "    # Compute and return the MSE of a trivial model that always returns the global mean computed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bde7d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alphaUpdate(ratingsTrain, alpha, betaU, betaI, lamb):\n",
    "    # Update equation for alpha\n",
    "    return newAlpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d685a57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def betaUUpdate(ratingsPerUser, alpha, betaU, betaI, lamb):\n",
    "    # Update equation for betaU\n",
    "    return newBetaU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d3af8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def betaIUpdate(ratingsPerItem, alpha, betaU, betaI, lamb):\n",
    "    # Update equation for betaI\n",
    "    return newBetaI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c109250d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def msePlusReg(ratingsTrain, alpha, betaU, betaI, lamb):\n",
    "    # Compute the MSE and the mse+regularization term\n",
    "    return mse, mse + lamb*regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d6dc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validMSE(ratingsValid, alpha, betaU, betaI):\n",
    "    # Compute the MSE on the validation set\n",
    "    return validMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388056f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def goodModel(ratingsTrain, ratingsPerUser, ratingsPerItem, alpha, betaU, betaI):\n",
    "    # Improve upon your model from the previous question (e.g. by running multiple iterations)\n",
    "    return alpha, betaU, betaI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a670321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writePredictionsRating(alpha, betaU, betaI):\n",
    "    # Write your predictions to a file that you can submit\n",
    "    predictions = open(\"predictions_Rating.csv\", 'w')\n",
    "    for l in open(\"pairs_Rating.csv\"):\n",
    "        if l.startswith(\"userID\"):\n",
    "            predictions.write(l)\n",
    "            continue\n",
    "        u,b = l.strip().split(',')\n",
    "        bu = 0\n",
    "        bi = 0\n",
    "        if u in betaU:\n",
    "            bu = betaU[u]\n",
    "        if b in betaI:\n",
    "            bi = betaI[b]\n",
    "        _ = predictions.write(u + ',' + b + ',' + str(alpha + bu + bi) + '\\n')\n",
    "\n",
    "    predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd75a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Read prediction                                #\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04946ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateValidation(allRatings, ratingsValid):\n",
    "    # Using ratingsValid, generate two sets:\n",
    "    # readValid: set of (u,b) pairs in the validation set\n",
    "    # notRead: set of (u,b') pairs, containing one negative (not read) for each row (u) in readValid  \n",
    "    # Both should have the same size as ratingsValid\n",
    "    return readValid, notRead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdf7f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseLineStrategy(mostPopular, totalRead):\n",
    "    return1 = set()\n",
    "\n",
    "    # Compute the set of items for which we should return \"True\"\n",
    "    # This is the same strategy implemented in the baseline code for Assignment 1\n",
    "    return1 = set()\n",
    "    count = 0\n",
    "    for ic, i in mostPopular:\n",
    "        count += ic\n",
    "        return1.add(i)\n",
    "        if count > totalRead/2: break\n",
    "\n",
    "    return return1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c7c66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improvedStrategy(mostPopular, totalRead):\n",
    "    return1 = set()\n",
    "\n",
    "    # Same as above function, just find an item set that'll have higher accuracy\n",
    "\n",
    "    return return1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879652f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateStrategy(return1, readValid, notRead):\n",
    "\n",
    "    # Compute the accuracy of a strategy which just returns \"true\" for a set of items (those in return1)\n",
    "    # readValid: instances with positive label\n",
    "    # notRead: instances with negative label\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acbf0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccardThresh(u,b,ratingsPerItem,ratingsPerUser):\n",
    "    \n",
    "    # Compute the similarity of the query item (b) compared to the most similar item in the user's history\n",
    "    # Return true if the similarity is high or the item is popular\n",
    "    \n",
    "    if maxSim > 0.013 or len(ratingsPerItem[b]) > 40: # Keep these thresholds as-is\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13664877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writePredictionsRead(ratingsPerItem, ratingsPerUser):\n",
    "    predictions = open(\"predictions_Read.csv\", 'w')\n",
    "    for l in open(\"pairs_Read.csv\"):\n",
    "        if l.startswith(\"userID\"):\n",
    "            predictions.write(l)\n",
    "            continue\n",
    "        u,b = l.strip().split(',')\n",
    "        pred = jaccardThresh(u,b,ratingsPerItem,ratingsPerUser)\n",
    "        _ = predictions.write(u + ',' + b + ',' + str(pred) + '\\n')\n",
    "\n",
    "    predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1e31f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Category prediction                            #\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445775f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureCat(datum, words, wordId, wordSet):\n",
    "    feat = [0]*len(words)\n",
    "\n",
    "    # Compute features counting instance of each word in \"words\"\n",
    "    # after converting to lower case and removing punctuation\n",
    "    \n",
    "    feat.append(1) # offset (put at the end)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4985733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def betterFeatures(data):\n",
    "    \n",
    "    # Produce better features than those from the above question\n",
    "    # Return matrix (each row is the feature vector for one entry in the dataset)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e24732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runOnTest(data_test, mod):\n",
    "    Xtest = [featureCat(d) for d in data_test]\n",
    "    pred_test = mod.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72359012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writePredictionsCategory(pred_test):\n",
    "    predictions = open(\"predictions_Category.csv\", 'w')\n",
    "    pos = 0\n",
    "\n",
    "    for l in open(\"pairs_Category.csv\"):\n",
    "        if l.startswith(\"userID\"):\n",
    "            predictions.write(l)\n",
    "            continue\n",
    "        u,b = l.strip().split(',')\n",
    "        _ = predictions.write(u + ',' + b + ',' + str(pred_test[pos]) + '\\n')\n",
    "        pos += 1\n",
    "\n",
    "    predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681bd24a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
