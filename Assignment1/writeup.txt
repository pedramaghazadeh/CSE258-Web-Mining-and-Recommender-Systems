PREDICT READ TASK WRITEUP

For the read-prediction task, I began with the baseline approach from HW3, which focuses on identifying whether a user will read a given book based purely on interaction frequencies. The baseline relies on the observation that the popularity of a book alone provides surprisingly strong predictive power. I reproduced this baseline first, ensuring that I matched the expected validation setup using a 50–50 split of positive (read) and synthetic negative (not-read) samples. From there, I tuned the strategy for improved accuracy by incorporating a cumulative popularity threshold that selects a subset of the most frequently interacted books until a fixed proportion of the total reads is covered. This simple popularity-based decision rule provided a strong baseline and met the required performance constraints.

Once the basic version was stable, I experimented with more advanced signals, such as Jaccard similarity between items a user has interacted with and the candidate item. This involved computing user–item histories and evaluating cross-item overlap via shared readers. While this similarity metric improved interpretability and occasionally helped on specific users, it did not outperform the optimized popularity threshold, especially once the validation set was balanced between positives and negatives. Ultimately, the improved popularity-based strategy produced robust validation accuracy while requiring no additional parameter tuning or model complexity.

For leaderboard experimentation, I tried several variants including hybrid popularity-plus-similarity rules, user-specific thresholds, and item frequency filtering. However, these approaches tended to overfit or introduce noise relative to the very strong simplicity of the original method. In the final submitted version, I kept the optimized popularity-based “improvedStrategy” as the prediction rule, as it consistently produced the best overall performance compared to more complex alternatives.

⸻

PREDICT CATEGORY TASK WRITEUP

The category prediction task required modeling the genre of each review using the review text itself. I started with the baseline TF-IDF + Logistic Regression pipeline and incrementally enhanced each component. The first major improvement was extending the preprocessing pipeline to include lowercasing, punctuation removal, stop-word filtering, and optional stemming or lemmatization. Applying these steps consistently helped reduce noise in the textual inputs, especially because the dataset contains many informal phrases and mixed casing. I then expanded from unigram features to a more expressive TF-IDF representation including word n-grams up to length 3 and additional character n-grams, which helped capture stylistic and morphological patterns across genres.

The combined feature space was constructed using a FeatureUnion of word-based and character-based TF-IDF vectorizers. This hybrid approach proved robust, as character features help correct for misspellings and rare words while word n-grams preserve semantic structure. To ensure well-conditioned optimization, I normalized the feature vectors and trained a logistic regression classifier with L2 regularization, balanced class weights, and an increased maximum iteration budget to guarantee convergence on the high-dimensional data. A validation split confirmed that the expanded model significantly outperformed simpler TF-IDF settings.

For leaderboard exploration, I tested various regularization strengths, alternate solvers, class-rebalancing strategies, and dimensionality caps, as well as experiments with stemming/lemmatization. Some configurations improved training performance but hurt generalization, confirming that the original balanced logistic regression setup was a strong choice. The final submitted model used the optimized TF-IDF configuration and the tuned logistic regression classifier, which produced stable and competitive validation accuracy without added computational overhead.

⸻

PREDICT RATING TASK WRITEUP

The rating-prediction task was framed as a classic bias-based matrix-factorization style model without latent factors, matching the baseline for HW3. The model predicts ratings using a global average term combined with user-specific and item-specific bias parameters. I first reproduced the baseline by computing initial averages and applying iterative updates for alpha (global bias), betaU (user bias), and betaI (item bias). These parameters were optimized using alternating updates that minimize squared error with L2 regularization. Early validation confirmed the expected convergence behavior and demonstrated that bias-only models can achieve strong performance with relatively few parameters.

To improve the model, I ran several hundred training iterations while tracking validation MSE and preserving the best parameter set encountered. Regularization strength played an important role, as overly small values caused overfitting while large values suppressed useful bias patterns. After extensive testing, I found that lambda=5 and approximately 250 update iterations provided a stable balance. The model was further enhanced by implementing a principled fallback system for unseen users or items in the test set. Specifically, I used a damped user-average estimate for cold-start items and the global average for full cold-start cases. This significantly improved the model’s ability to generalize to unseen inputs.

For leaderboard experimentation, I tested latent-factor matrix factorization, Surprise-SVD, and neural models. Although these approaches achieved lower training error, they performed worse on validation MSE due to overfitting on the highly sparse ratings matrix. Ultimately, the tuned bias-only model remained the most stable and effective across both validation and leaderboard performance. Its simplicity, interpretability, and robustness made it the best-performing choice for the final submission.
