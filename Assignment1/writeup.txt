PREDICT READ TASK WRITEUP

For the read-prediction task, I originally experimented with multiple baseline heuristics from HW3, including popularity-based strategies and Jaccard similarity rules. Although these approaches were simple and surprisingly competitive, their best performance plateaued around the mid-0.70s in the leaderboard. To significantly improve beyond that, I implemented a full latent-factor model inspired by Bayesian Personalized Ranking (BPR). The idea behind BPR is to learn user–item preference rankings by contrasting positive user–book interactions against randomly sampled negative (unread) books. I trained 20-dimensional user and item embeddings using gradient updates derived from the BPR objective, combined with learned user and item biases and L2 regularization for stability. A held-out validation set composed of positive/negative sampled pairs was used each epoch to compute ranking accuracy (AUC) and to monitor convergence.

After obtaining the learned latent factors, I developed a scoring function that combined the embedding-based preference score with two additional heuristic boosts: Jaccard similarity between the candidate book and the user’s reading history, and a small popularity-based log-count adjustment. These heuristics were weighted lightly so that the model remained primarily driven by latent factors but still benefited from signals that BPR alone can struggle to capture. To convert scores into binary predictions, I explored both global thresholds (chosen by validation accuracy) and per-user calibrated thresholds. The latter—predicting the top 50% highest-scoring items per user—substantially improved prediction balance and reduced user-level bias introduced by the long-tail distribution of books.

The combined model (latent factors + biases + heuristic features + per-user thresholding) produced a sizable improvement on the leaderboard, increasing my score from roughly 0.76 to about 0.82. This result reflects the strength of collaborative filtering with pairwise ranking objectives on implicit feedback data, especially when augmented with domain-specific heuristics and careful prediction calibration. Although the model is significantly more complex than the initial baselines, it generalized better and delivered the highest accuracy among all the methods I tested.
⸻

PREDICT CATEGORY TASK WRITEUP

The category prediction task required modeling the genre of each review using the review text itself. I started with the baseline TF-IDF + Logistic Regression pipeline and incrementally enhanced each component. The first major improvement was extending the preprocessing pipeline to include lowercasing, punctuation removal, stop-word filtering, and optional stemming or lemmatization. Applying these steps consistently helped reduce noise in the textual inputs, especially because the dataset contains many informal phrases and mixed casing. I then expanded from unigram features to a more expressive TF-IDF representation including word n-grams up to length 3 and additional character n-grams, which helped capture stylistic and morphological patterns across genres.

The combined feature space was constructed using a FeatureUnion of word-based and character-based TF-IDF vectorizers. This hybrid approach proved robust, as character features help correct for misspellings and rare words while word n-grams preserve semantic structure. To ensure well-conditioned optimization, I normalized the feature vectors and trained a logistic regression classifier with L2 regularization, balanced class weights, and an increased maximum iteration budget to guarantee convergence on the high-dimensional data. A validation split confirmed that the expanded model significantly outperformed simpler TF-IDF settings.

For leaderboard exploration, I tested various regularization strengths, alternate solvers, class-rebalancing strategies, and dimensionality caps, as well as experiments with stemming/lemmatization. Some configurations improved training performance but hurt generalization, confirming that the original balanced logistic regression setup was a strong choice. The final submitted model used the optimized TF-IDF configuration and the tuned logistic regression classifier, which produced stable and competitive validation accuracy without added computational overhead.

⸻

PREDICT RATING TASK WRITEUP

The rating-prediction task was framed as a classic bias-based matrix-factorization style model without latent factors, matching the baseline for HW3. The model predicts ratings using a global average term combined with user-specific and item-specific bias parameters. I first reproduced the baseline by computing initial averages and applying iterative updates for alpha (global bias), betaU (user bias), and betaI (item bias). These parameters were optimized using alternating updates that minimize squared error with L2 regularization. Early validation confirmed the expected convergence behavior and demonstrated that bias-only models can achieve strong performance with relatively few parameters.

To improve the model, I ran several hundred training iterations while tracking validation MSE and preserving the best parameter set encountered. Regularization strength played an important role, as overly small values caused overfitting while large values suppressed useful bias patterns. After extensive testing, I found that lambda=5 and approximately 250 update iterations provided a stable balance. The model was further enhanced by implementing a principled fallback system for unseen users or items in the test set. Specifically, I used a damped user-average estimate for cold-start items and the global average for full cold-start cases. This significantly improved the model’s ability to generalize to unseen inputs.

For leaderboard experimentation, I tested latent-factor matrix factorization, Surprise-SVD, and neural models. Although these approaches achieved lower training error, they performed worse on validation MSE due to overfitting on the highly sparse ratings matrix. Ultimately, the tuned bias-only model remained the most stable and effective across both validation and leaderboard performance. Its simplicity, interpretability, and robustness made it the best-performing choice for the final submission.
